{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba51b8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multispeaker-clean-vits-anwar-ibrahim.json',\n",
       " 'multispeaker-clean-vits-kp-ms.json',\n",
       " 'multispeaker-clean-vits-shafiqah-idayu-chatbot.json',\n",
       " 'multispeaker-clean-vits-husein-chatbot.json',\n",
       " 'multispeaker-clean-vits-kp-zh.json']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "files = glob('*vits*.json')\n",
    "files = [f for f in files if 'combine' not in f and 'multispeaker-clean-vits.json' not in f]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e235bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multispeaker-clean-vits-anwar-ibrahim.json 106014\n",
      "multispeaker-clean-vits-kp-ms.json 160265\n",
      "multispeaker-clean-vits-shafiqah-idayu-chatbot.json 141475\n",
      "multispeaker-clean-vits-husein-chatbot.json 127137\n",
      "multispeaker-clean-vits-kp-zh.json 111128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "646019"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_all = []\n",
    "for f in files:\n",
    "    with open(f) as fopen:\n",
    "        d = json.load(fopen)\n",
    "    print(f, len(d))\n",
    "    combine_all.extend(d)\n",
    "    \n",
    "len(combine_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59980ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anwar-ibrahim-chatbot': 8,\n",
       " 'kp-ms-chatbot': 8,\n",
       " 'shafiqah-idayu-chatbot': 10,\n",
       " 'husein-chatbot': 10,\n",
       " 'kp-zh-chatbot': 8}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speakers = {}\n",
    "for i in range(len(combine_all)):\n",
    "    speakers[combine_all[i][0].split('tts/')[1].split('/')[0]] = combine_all[i][1]\n",
    "    \n",
    "speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e420a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_speakers = {\n",
    "    'anwar-ibrahim-chatbot': 'Anwar Ibrahim',\n",
    "    'shafiqah-idayu-chatbot': 'Shafiqah Idayu',\n",
    "    'husein-chatbot': 'Husein',\n",
    "    'kp-ms-chatbot': 'KP',\n",
    "    'kp-zh-chatbot': 'KP',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c74e422c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Walaupun secara teorinya mungkin untuk membina struktur kecil dengan pencungkil gigi , ia agak mencabar dan tidak sesuai untuk rumah bersaiz biasa .eos'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "_pad = 'pad'\n",
    "_start = 'start'\n",
    "_eos = 'eos'\n",
    "_punctuation = \"!'(),.:;? \"\n",
    "_special = '-'\n",
    "_letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "MALAYA_SPEECH_SYMBOLS = (\n",
    "    [_pad, _start, _eos] + list(_special) + list(_punctuation) + list(_letters)\n",
    ")\n",
    "''.join([MALAYA_SPEECH_SYMBOLS[i] for i in combine_all[1][2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f7a517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 646019/646019 [00:01<00:00, 395009.10it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(combine_all))):\n",
    "    new_id = new_speakers[combine_all[i][0].split('tts/')[1].split('/')[0]]\n",
    "    combine_all[i][1] = new_id\n",
    "    if isinstance(combine_all[i][2], list):\n",
    "        combine_all[i][2] = ''.join([MALAYA_SPEECH_SYMBOLS[c] for c in combine_all[i][2][:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49157ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(646019, 646019)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = defaultdict(int)\n",
    "for d in combine_all:\n",
    "    files[d[0]] += 1\n",
    "    \n",
    "len(files), len(combine_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf5b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in files.items():\n",
    "    if v > 1:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aae1f353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/husein/ssd3/tts/kp-zh-chatbot/kp-chinese-texts-part4-7824.wav',\n",
       " 'KP',\n",
       " '今天呢我们开始讲一本书 , 叫你是孩子最好的玩具 .']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_all[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c8da07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.311 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "from pypinyin import lazy_pinyin, Style\n",
    "import jieba\n",
    "\n",
    "jieba.initialize()\n",
    "\n",
    "def is_chinese(c):\n",
    "    return (\n",
    "        \"\\u3100\" <= c <= \"\\u9fff\"\n",
    "    )\n",
    "\n",
    "def convert_char_to_pinyin(text_list, polyphone=True):\n",
    "    final_text_list = []\n",
    "    custom_trans = str.maketrans(\n",
    "        {\";\": \",\", \"“\": '\"', \"”\": '\"', \"‘\": \"'\", \"’\": \"'\", '，': ', ', '！': '. ', '。': '. '}\n",
    "    ) \n",
    "\n",
    "    for text in text_list:\n",
    "        char_list = []\n",
    "        text = text.translate(custom_trans)\n",
    "        for seg in jieba.cut(text):\n",
    "            seg_byte_len = len(bytes(seg, \"UTF-8\"))\n",
    "            if seg_byte_len == len(seg):  # if pure alphabets and symbols\n",
    "                if char_list and seg_byte_len > 1 and char_list[-1] not in \" :'\\\"\":\n",
    "                    char_list.append(\" \")\n",
    "                char_list.extend(seg)\n",
    "            elif polyphone and seg_byte_len == 3 * len(seg):  # if pure east asian characters\n",
    "                seg_ = lazy_pinyin(seg, style=Style.TONE3, tone_sandhi=True)\n",
    "                for i, c in enumerate(seg):\n",
    "                    if is_chinese(c):\n",
    "                        char_list.append(\" \")\n",
    "                    char_list.append(seg_[i])\n",
    "            else:  # if mixed characters, alphabets and symbols\n",
    "                for c in seg:\n",
    "                    if ord(c) < 256:\n",
    "                        char_list.extend(c)\n",
    "                    elif is_chinese(c):\n",
    "                        char_list.append(\" \")\n",
    "                        char_list.extend(lazy_pinyin(c, style=Style.TONE3, tone_sandhi=True))\n",
    "                    else:\n",
    "                        char_list.append(c)\n",
    "        final_text_list.append(char_list)\n",
    "\n",
    "    return final_text_list\n",
    "\n",
    "def normalize(text):\n",
    "    converted = convert_char_to_pinyin(text.split())\n",
    "    converted = [''.join(c) for c in converted]\n",
    "    return ' '.join(converted).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d61e9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 646019/646019 [02:02<00:00, 5254.95it/s]\n"
     ]
    }
   ],
   "source": [
    "speakers = defaultdict(list)\n",
    "for r in tqdm(combine_all):\n",
    "    a = {\n",
    "        'audio_filename': r[0],\n",
    "        'prompt': r[1],\n",
    "        'transcription': normalize(r[2])\n",
    "    }\n",
    "    speakers[r[1]].append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c182240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = [], []\n",
    "for k, v in speakers.items():\n",
    "    train_, test_ = train_test_split(v, test_size = 10)\n",
    "    train.extend(train_)\n",
    "    test.extend(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40448f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train)\n",
    "random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27d6737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_list(train),\n",
    "    'test': Dataset.from_list(test)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70402429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio_filename', 'prompt', 'transcription'],\n",
       "        num_rows: 645979\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio_filename', 'prompt', 'transcription'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d0cb1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_filename': '/home/husein/ssd3/tts/husein-chatbot/husein-chatbot-normalized-v2-54278.wav',\n",
       " 'prompt': 'Husein',\n",
       " 'transcription': 'Pokok tidak mempunyai rasa , kerana ia tidak boleh dimakan .'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df7162b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b399a72a3f2452397c31a52a3bd05d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa2789630654c2a8597faa2ee4d8bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/646 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c884d8c2f59c4556ae2f5970e486fa79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d2dc67be7d438f9f78f0889abb9e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7142ec24122d48c984c9faae0a9ad807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/476 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/huseinzol05/mesolitica-tts-combined/commit/bf486f8397018d1668e4344d3b09009d1fa2efda', commit_message='Upload dataset', commit_description='', oid='bf486f8397018d1668e4344d3b09009d1fa2efda', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/huseinzol05/mesolitica-tts-combined', endpoint='https://huggingface.co', repo_type='dataset', repo_id='huseinzol05/mesolitica-tts-combined'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict.push_to_hub('huseinzol05/mesolitica-tts-combined')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
