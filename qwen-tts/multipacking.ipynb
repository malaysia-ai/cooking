{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23239631-e47d-40c3-b6ab-e07b3332c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id=\"malaysia-ai/Multilingual-TTS\", \n",
    "#     repo_type=\"dataset\",\n",
    "#     allow_patterns=\"*/*.parquet\",\n",
    "#     local_dir=\"./Multilingual-TTS\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4589826-534e-4a79-ab4b-6f4f5aebea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snapshot_download(\n",
    "#     repo_id=\"malaysia-ai/Multilingual-TTS\", \n",
    "#     repo_type=\"dataset\",\n",
    "#     allow_patterns=\"*_neucodec.zip\",\n",
    "#     local_dir=\"./\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "590013d6-636c-4aa7-9cc0-04902247a073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import IPython.display as ipd\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from transformers import AutoTokenizer, AddedToken\n",
    "from streaming import MDSWriter\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from streaming import LocalDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocess import Pool\n",
    "import itertools\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield (l[i: i + n], i // n)\n",
    "\n",
    "def multiprocessing(strings, function, cores=6, returned=True):\n",
    "    df_split = chunks(strings, len(strings) // cores)\n",
    "    pool = Pool(cores)\n",
    "    pooled = pool.map(function, df_split)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    if returned:\n",
    "        return list(itertools.chain(*pooled))\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "columns = {\n",
    "    'input_ids': 'uint32',\n",
    "    'position_ids': 'uint32',\n",
    "    'attention_mask': 'uint32',\n",
    "    'audio': 'str',\n",
    "    'text': 'str'\n",
    "}\n",
    "hashes = 'sha1', 'xxh64'\n",
    "\n",
    "def new_path(f):\n",
    "    splitted = f.split('/')\n",
    "    folder = f.split('/')[0]\n",
    "    folder = folder + '_neucodec'\n",
    "    new_f = os.path.join(folder, '/'.join(splitted[1:]))\n",
    "    new_f = new_f.replace('.mp3', '.json').replace('.wav', '.json')\n",
    "    return new_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d9ea460-dfb1-4a22-81be-250f9bcbc1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(files):\n",
    "    files, _ = files\n",
    "    data = []\n",
    "    for f in tqdm(files):\n",
    "        df = pd.read_parquet(f).to_dict(orient = 'records')\n",
    "        for i in range(len(df)):\n",
    "            token_filename = new_path(df[i]['audio_filename'])\n",
    "            if not os.path.exists(token_filename):\n",
    "                continue\n",
    "            df[i]['token_filename'] = token_filename\n",
    "            data.append(df[i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440282b7-3ac8-47c8-9390-44d8a1aad75c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = glob('Multilingual-TTS/*/*.parquet')\n",
    "files = [f for f in files if 'Malaysian-TTS-v2' not in f]\n",
    "data = multiprocessing(files, loop, cores = 20)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f003aea-a640-4797-b18e-5b610674bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "malaysian = pd.read_parquet('Multilingual-TTS/Malaysian-TTS-v2/train-00000-of-00001.parquet').to_dict(orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba693f9-caae-4c04-88ae-83032ef2364e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65537"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-1.7B-Base')\n",
    "extra = [AddedToken('<|speech_start|>')]\n",
    "for i in range(65536):\n",
    "    extra.append(AddedToken(f'<|s_{i}|>'))\n",
    "tokenizer.add_tokens(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff1795-d647-4955-b8a0-62f7dc899b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def collator(batch, batch_position_ids):\n",
    "    input_ids = []\n",
    "    position_ids = []\n",
    "    masks = []\n",
    "    for i in range(len(batch)):\n",
    "        l = len(batch[i])\n",
    "        input_ids.extend(batch[i])\n",
    "        position_ids.extend(batch_position_ids[i])\n",
    "        masks.append(l)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(input_ids).astype(np.uint32),\n",
    "        'position_ids': np.array(position_ids).astype(np.uint32),\n",
    "        'attention_mask': np.array(masks).astype(np.uint32),\n",
    "        'audio': '',\n",
    "        'text': '',\n",
    "    }\n",
    "\n",
    "def slice_and_balance(nested_list, size):\n",
    "    first = []\n",
    "    balance = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sublist in nested_list:\n",
    "        if current_size < size:\n",
    "            remaining_space = size - current_size\n",
    "            if len(sublist) <= remaining_space:\n",
    "                first.append(sublist)\n",
    "                current_size += len(sublist)\n",
    "            else:\n",
    "                first.append(sublist[:remaining_space])\n",
    "                balance.append(sublist[remaining_space:])\n",
    "                current_size = size\n",
    "        else:\n",
    "            balance.append(sublist)\n",
    "    \n",
    "    return first, balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7092139-3e14-4d5b-8492-644dd4758418",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tokenized-4k-qwen3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace023d3-12fb-433b-9143-89c683dc628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sequence_length = 1024 * 10\n",
    "def loop(files, block_size = sequence_length):\n",
    "    rows, index = files\n",
    "    out_root = f'tokenized-4k-qwen3/tokenized-{index}'\n",
    "    os.system(f'rm -rf {out_root}')\n",
    "    count = 0\n",
    "    temp = []\n",
    "    position_ids = []\n",
    "    last_block, last_position_block = None, None\n",
    "    with MDSWriter(out=out_root, columns=columns, compression=None, hashes=hashes) as out:\n",
    "        for row in tqdm(rows):\n",
    "\n",
    "            text = row['text']\n",
    "            \n",
    "            try:\n",
    "                with open(row['token_filename']) as fopen:\n",
    "                    token = json.load(fopen)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if len(text.split()) > len(token):\n",
    "                continue\n",
    "\n",
    "            left = row['speaker'] +': ' + text\n",
    "            \n",
    "            token = ''.join([f'<|s_{t}|>' for t in token])\n",
    "            prompt = f'<|im_start|>{left}<|speech_start|>{token}<|im_end|>'\n",
    "            \n",
    "            outputs = tokenizer(prompt, add_special_tokens = False)\n",
    "            position = range(len(outputs['input_ids']))\n",
    "            length = len(outputs['input_ids'])\n",
    "            \n",
    "            if count + length > block_size:\n",
    "                o = collator(temp, position_ids)\n",
    "                if o['input_ids'].shape[0] > 0:\n",
    "                    out.write(o)\n",
    "                temp = [outputs['input_ids']]\n",
    "                position_ids = [position]\n",
    "                count = length\n",
    "                \n",
    "            else:\n",
    "                temp.append(outputs['input_ids'])\n",
    "                position_ids.append(range(len(outputs['input_ids'])))\n",
    "                count += len(outputs['input_ids'])\n",
    "        \n",
    "        if len(temp):\n",
    "            o = collator(temp, position_ids)\n",
    "            if o['input_ids'].shape[0] > 0:\n",
    "                out.write(o)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfbc9d1-f325-4fba-b52c-727560b32538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop((data[:10], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd8128-49fa-4540-94fe-6d3784b0ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = data + malaysian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c363faf-8d35-4d81-bf90-6268624c6706",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing(combined, loop, cores = 40, returned = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee6cdb01-2be3-4b03-b380-c9b75142a68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenized-4k-qwen3/tokenized-0',\n",
       " 'tokenized-4k-qwen3/tokenized-1',\n",
       " 'tokenized-4k-qwen3/tokenized-2',\n",
       " 'tokenized-4k-qwen3/tokenized-3',\n",
       " 'tokenized-4k-qwen3/tokenized-4',\n",
       " 'tokenized-4k-qwen3/tokenized-5',\n",
       " 'tokenized-4k-qwen3/tokenized-6',\n",
       " 'tokenized-4k-qwen3/tokenized-7',\n",
       " 'tokenized-4k-qwen3/tokenized-8',\n",
       " 'tokenized-4k-qwen3/tokenized-9',\n",
       " 'tokenized-4k-qwen3/tokenized-10',\n",
       " 'tokenized-4k-qwen3/tokenized-11',\n",
       " 'tokenized-4k-qwen3/tokenized-12',\n",
       " 'tokenized-4k-qwen3/tokenized-13',\n",
       " 'tokenized-4k-qwen3/tokenized-14',\n",
       " 'tokenized-4k-qwen3/tokenized-15',\n",
       " 'tokenized-4k-qwen3/tokenized-16',\n",
       " 'tokenized-4k-qwen3/tokenized-17',\n",
       " 'tokenized-4k-qwen3/tokenized-18',\n",
       " 'tokenized-4k-qwen3/tokenized-19',\n",
       " 'tokenized-4k-qwen3/tokenized-20',\n",
       " 'tokenized-4k-qwen3/tokenized-21',\n",
       " 'tokenized-4k-qwen3/tokenized-22',\n",
       " 'tokenized-4k-qwen3/tokenized-23',\n",
       " 'tokenized-4k-qwen3/tokenized-24',\n",
       " 'tokenized-4k-qwen3/tokenized-25',\n",
       " 'tokenized-4k-qwen3/tokenized-26',\n",
       " 'tokenized-4k-qwen3/tokenized-27',\n",
       " 'tokenized-4k-qwen3/tokenized-28',\n",
       " 'tokenized-4k-qwen3/tokenized-29',\n",
       " 'tokenized-4k-qwen3/tokenized-30',\n",
       " 'tokenized-4k-qwen3/tokenized-31',\n",
       " 'tokenized-4k-qwen3/tokenized-32',\n",
       " 'tokenized-4k-qwen3/tokenized-33',\n",
       " 'tokenized-4k-qwen3/tokenized-34',\n",
       " 'tokenized-4k-qwen3/tokenized-35',\n",
       " 'tokenized-4k-qwen3/tokenized-36',\n",
       " 'tokenized-4k-qwen3/tokenized-37',\n",
       " 'tokenized-4k-qwen3/tokenized-38',\n",
       " 'tokenized-4k-qwen3/tokenized-39',\n",
       " 'tokenized-4k-qwen3/tokenized-40']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = sorted(glob('tokenized-4k-qwen3/tokenized-*'), key = lambda x: int(x.split('-')[-1]))\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c7348a9-0a03-49e8-86a9-3581e1c37532",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf multipacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4b74b81-7201-478d-a50f-7522872255b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13270/13270 [00:02<00:00, 5219.32it/s]\n",
      "100%|██████████| 12010/12010 [00:02<00:00, 5136.74it/s]\n",
      "100%|██████████| 9434/9434 [00:01<00:00, 5118.57it/s]\n",
      "100%|██████████| 12602/12602 [00:02<00:00, 4934.11it/s]\n",
      "100%|██████████| 15388/15388 [00:03<00:00, 5063.01it/s]\n",
      "100%|██████████| 20451/20451 [00:03<00:00, 5748.51it/s]\n",
      "100%|██████████| 20449/20449 [00:03<00:00, 5589.55it/s]\n",
      "100%|██████████| 19496/19496 [00:03<00:00, 5769.34it/s]\n",
      "100%|██████████| 20825/20825 [00:03<00:00, 5667.95it/s]\n",
      "100%|██████████| 23834/23834 [00:08<00:00, 2958.19it/s]\n",
      "100%|██████████| 15024/15024 [00:02<00:00, 5259.25it/s]\n",
      "100%|██████████| 11703/11703 [00:02<00:00, 5036.67it/s]\n",
      "100%|██████████| 10998/10998 [00:02<00:00, 5109.05it/s]\n",
      "100%|██████████| 12056/12056 [00:02<00:00, 5267.46it/s]\n",
      "100%|██████████| 10493/10493 [00:02<00:00, 4788.72it/s]\n",
      "100%|██████████| 10541/10541 [00:02<00:00, 4782.85it/s]\n",
      "100%|██████████| 10675/10675 [00:02<00:00, 4820.66it/s]\n",
      "100%|██████████| 11010/11010 [00:02<00:00, 4583.10it/s]\n",
      "100%|██████████| 10250/10250 [00:01<00:00, 5309.90it/s]\n",
      "100%|██████████| 10118/10118 [00:03<00:00, 3293.52it/s]\n",
      "100%|██████████| 9835/9835 [00:04<00:00, 2164.97it/s]\n",
      "100%|██████████| 8788/8788 [00:01<00:00, 5036.33it/s]\n",
      "100%|██████████| 9901/9901 [00:01<00:00, 5146.53it/s]\n",
      "100%|██████████| 9684/9684 [00:01<00:00, 4900.10it/s]\n",
      "100%|██████████| 10432/10432 [00:01<00:00, 5463.72it/s]\n",
      "100%|██████████| 10455/10455 [00:01<00:00, 5288.25it/s]\n",
      "100%|██████████| 9255/9255 [00:01<00:00, 5281.18it/s]\n",
      "100%|██████████| 9402/9402 [00:01<00:00, 5167.93it/s]\n",
      "100%|██████████| 9770/9770 [00:01<00:00, 5238.89it/s]\n",
      "100%|██████████| 10493/10493 [00:01<00:00, 5425.29it/s]\n",
      "100%|██████████| 10091/10091 [00:02<00:00, 4964.06it/s]\n",
      "100%|██████████| 13210/13210 [00:02<00:00, 5495.50it/s]\n",
      "100%|██████████| 10622/10622 [00:02<00:00, 4509.50it/s]\n",
      "100%|██████████| 11258/11258 [00:02<00:00, 5290.04it/s]\n",
      "100%|██████████| 16034/16034 [00:02<00:00, 5420.82it/s]\n",
      "100%|██████████| 12481/12481 [00:02<00:00, 5153.24it/s]\n",
      "100%|██████████| 11282/11282 [00:02<00:00, 4938.49it/s]\n",
      "100%|██████████| 15446/15446 [00:03<00:00, 5118.00it/s]\n",
      "100%|██████████| 13707/13707 [00:02<00:00, 5875.47it/s]\n",
      "100%|██████████| 14488/14488 [00:03<00:00, 4473.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3123.09it/s]\n"
     ]
    }
   ],
   "source": [
    "with MDSWriter(out='multipacking', columns=columns, compression=None, hashes=hashes) as out:\n",
    "    for f in folders:\n",
    "        try:\n",
    "            dataset = LocalDataset(local=f)\n",
    "            for i in tqdm(range(len(dataset))):\n",
    "                out.write(dataset[i])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "410d1c9e-aaf4-43ee-a596-4fbc0cf9d064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507262"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = LocalDataset('multipacking')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277982a2-2f97-46bf-93b8-7e30fb7d0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(dataset[255075]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64199da7-116e-4a29-9cbe-700eb1bdccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(len(dataset))):\n",
    "#     if dataset[i]['attention_mask'].max() > 9000:\n",
    "#         print(i, dataset[i]['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625be284-16bc-4276-9d0f-ed0ee615e45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['attention_mask'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6da4ee-705e-427a-b4d0-58a670db5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tokenizer.decode(dataset[255076]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4756b3-4e96-43ec-a85c-eff5d06c02fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = s.split('<|im_end|>')[0].split('<|speech_start|>')\n",
    "splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ee09f-1a9b-4ae1-bec2-807027cf587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "numbers = re.findall(r's_(\\d+)', splitted[1])\n",
    "d = list(map(int, numbers))\n",
    "len(splitted[0].split()), len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f18c02-dc1d-4a01-a086-b7abfa9c7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(d) / 50) / len(splitted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409679ac-2865-4e78-8656-f7508d0350ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neucodec import NeuCodec\n",
    "import torch\n",
    "\n",
    "neucodec = NeuCodec.from_pretrained(\"neuphonic/neucodec\")\n",
    "_ = neucodec.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883f985-c0b9-4241-9a08-6cd86af3c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq_codes = torch.tensor(d)[None][None]\n",
    "recon = neucodec.decode_code(fsq_codes.cuda()).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c4927-60a0-41f9-a075-2dc6056a12b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(recon[0, 0], rate = 24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71487a78-a985-47be-bd8a-b3e4d9e694e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hf upload malaysia-ai/multipacking-multilingual-tts-10k-qwen3 multipacking --repo-type=dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
