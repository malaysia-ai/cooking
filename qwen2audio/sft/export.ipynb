{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a40bbb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = '/home/husein/ssd3'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f6c6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration, AutoConfig, AutoModelForCausalLM\n",
    "from transformers import TextStreamer\n",
    "from datasets import Audio\n",
    "import torch\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ea71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_class = Audio(sampling_rate=16000)\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n",
    "tokenizer = processor.tokenizer\n",
    "audio_token = \"<|AUDIO|>\"\n",
    "audio_bos_token = \"<|audio_bos|>\"\n",
    "audio_eos_token = \"<|audio_eos|>\"\n",
    "audio_token_id = processor.tokenizer._convert_token_to_id_with_added_voc('<|AUDIO|>')\n",
    "pad_token_id = processor.tokenizer.pad_token_id\n",
    "torch_dtype = torch.bfloat16\n",
    "min_dtype = torch.finfo(torch_dtype).min\n",
    "sequence_length = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3cf87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "886daaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm adapter_model.safetensors\n",
    "# !wget https://huggingface.co/malayloraenjoyer/Malaysian-Qwen2-Audio-7B-Instruct-128/resolve/main/adapter_model.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b4d48b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac7e9f9db97480bba5469071a55934b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ori_model = Qwen2AudioForConditionalGeneration.from_pretrained(\n",
    "    'Qwen/Qwen2-Audio-7B-Instruct', torch_dtype = torch.bfloat16,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0551bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = ori_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b77fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "f = safe_open(f\"adapter_model.safetensors\", framework=\"pt\", device=0)\n",
    "keys = f.keys()\n",
    "keys = sorted(list(set([k.split('.lora')[0] for k in keys if '.lora' in k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ade25ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 226/226 [00:00<00:00, 228.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for k in tqdm(keys):\n",
    "    k_ori = k.replace('base_model.model.', '') + '.weight'\n",
    "    if 'embed_tokens' in k:\n",
    "        post_A = '.lora_embedding_A'\n",
    "        post_B = '.lora_embedding_B'\n",
    "    else:\n",
    "        post_A = '.lora_A.weight'\n",
    "        post_B = '.lora_B.weight'\n",
    "    A = k + post_A\n",
    "    B = k + post_B\n",
    "    \n",
    "    W = state_dict[k_ori]\n",
    "    if 'embed_tokens' not in k:\n",
    "        W = W.t()\n",
    "        \n",
    "    A = f.get_tensor(A).type(W.dtype)\n",
    "    B = f.get_tensor(B).type(W.dtype)\n",
    "    with torch.no_grad():\n",
    "        W.addmm_(A.t(), B.t(), alpha = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abae792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Audio 1: <|audio_bos|><|AUDIO|><|audio_eos|>\n",
      "transcribe it in whisper segment timestamps<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|startoftranscript|><|ms|><|transcribe|><|0.82|> Maka, memang mana nak<|1.52|><|2.16|> kod<|2.44|><|2.70|> website pakai<|4.04|><|4.14|> React?<|4.56|><|4.56|> (Makna) Maka, memang mana nak kod website pakai<|4.96|><|5.36|> React?<|7.38|><|7.74|> (Makna) Maka, memang mana nak kod website pakai<|5.64|><|5.98|> React?<|6.64|><|7.22|> (Makna) Maka, memang mana nak kod website pakai<|16.12|><|16.36|> React?<|9.46|><|9.84|> (Makna) Maka, memang mana nak kod website pakai<|16.60|><|16.72|> React?<|17.06|><|17.36|> (\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"speech/code-website-react.mp3\"},\n",
    "        {\"type\": \"text\", \"text\": \"transcribe it in whisper segment timestamps\"},\n",
    "    ]},\n",
    "]\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios = []\n",
    "for message in conversation:\n",
    "    if isinstance(message[\"content\"], list):\n",
    "        for ele in message[\"content\"]:\n",
    "            if ele[\"type\"] == \"audio\":\n",
    "                audios.append(librosa.load(ele['audio_url'], \n",
    "                    sr=processor.feature_extractor.sampling_rate)[0]\n",
    "                )\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "    generate_kwargs = dict(\n",
    "        max_new_tokens=128,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.05,\n",
    "        streamer=streamer,\n",
    "        **inputs,\n",
    "    )\n",
    "    generation_output = ori_model.generate(**generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2536f45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Audio 1: <|audio_bos|><|AUDIO|><|audio_eos|>\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Wix adalah platform web yang popular yang membolehkan anda membuat dan mengurus laman web dengan mudah, tidak seperti React yang merupakan kerangka kerja pembangunan aplikasi web.\n",
      "\n",
      "Untuk membuat laman web menggunakan Wix, ikuti langkah-langkah berikut:\n",
      "\n",
      "1. Buka halaman Wix dalam pelayar web anda.\n",
      "2. Klik \"Buat Laman Web\" untuk mula projek anda.\n",
      "3. Pilih template yang sesuai untuk laman web anda. Terdapat pelbagai pilihan template tersedia seperti blog, toko, portfolio, dan banyak lagi\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"speech/code-website-react.mp3\"},\n",
    "    ]},\n",
    "]\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios = []\n",
    "for message in conversation:\n",
    "    if isinstance(message[\"content\"], list):\n",
    "        for ele in message[\"content\"]:\n",
    "            if ele[\"type\"] == \"audio\":\n",
    "                audios.append(librosa.load(ele['audio_url'], \n",
    "                    sr=processor.feature_extractor.sampling_rate)[0]\n",
    "                )\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "    generate_kwargs = dict(\n",
    "        max_new_tokens=128,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        temperature=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.05,\n",
    "        streamer=streamer,\n",
    "        **inputs,\n",
    "    )\n",
    "    generation_output = ori_model.generate(**generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67527745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Audio 1: <|audio_bos|><|AUDIO|><|audio_eos|>\n",
      "transcribe it in whisper word timestamps<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|startoftranscript|><|ms|><|transcribe|><|0.82|> Walaupun mempunyai masyarakat yang pelbagai dan pelbagai bahasa,<|4.10|><|4.50|> mengapa sistem pendidikan Malaysia masih<|6.28|><|6.80|> kurang dalam melahirkan graduan yang berdaya saing<|8.72|><|9.18|> di peringkat global?<|11.58|><|11.98|> Di sini, kita ada 20% orang yang tidak belajar. Ini bermakna kita mempunyai 80% orang yang tidak belajar. Ini bermakna kita mempunyai 80% orang yang tidak belajar\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"speech/graduan.mp3\"},\n",
    "        {\"type\": \"text\", \"text\": \"transcribe it in whisper word timestamps\"},\n",
    "    ]},\n",
    "]\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios = []\n",
    "for message in conversation:\n",
    "    if isinstance(message[\"content\"], list):\n",
    "        for ele in message[\"content\"]:\n",
    "            if ele[\"type\"] == \"audio\":\n",
    "                audios.append(librosa.load(ele['audio_url'], \n",
    "                    sr=processor.feature_extractor.sampling_rate)[0]\n",
    "                )\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "    generate_kwargs = dict(\n",
    "        max_new_tokens=128,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        temperature=0.01,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.05,\n",
    "        streamer=streamer,\n",
    "        **inputs,\n",
    "    )\n",
    "    generation_output = ori_model.generate(**generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db8d28b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Audio 1: <|audio_bos|><|AUDIO|><|audio_eos|>\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Malaysia adalah sebuah negara berbilang etnik dan berbilang agama dengan populasi yang pelbagai terdiri daripada kumpulan Melayu, Cina, India, dan lain-lain kumpulan minoriti. Walaupun negara ini telah mencapai kemajuan ekonomi yang ketara dalam beberapa dekad kebelakangan ini, ia masih mempunyai salah satu kadar ketidaksamaan pendapatan tertinggi di Asia Tenggara.\n",
      "\n",
      "Menurut Bank Dunia, setakat 2019, Malaysia mempuny\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"speech/kemiskinan.mp3\"},\n",
    "    ]},\n",
    "]\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios = []\n",
    "for message in conversation:\n",
    "    if isinstance(message[\"content\"], list):\n",
    "        for ele in message[\"content\"]:\n",
    "            if ele[\"type\"] == \"audio\":\n",
    "                audios.append(librosa.load(ele['audio_url'], \n",
    "                    sr=processor.feature_extractor.sampling_rate)[0]\n",
    "                )\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "    generate_kwargs = dict(\n",
    "        max_new_tokens=128,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        temperature=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.05,\n",
    "        streamer=streamer,\n",
    "        **inputs,\n",
    "    )\n",
    "    generation_output = ori_model.generate(**generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f4b7e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Audio 1: <|audio_bos|><|AUDIO|><|audio_eos|>\n",
      "apa isu<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Isu yang dikemukakan oleh Kerajaan Anwar Ibrahim ialah mengenai isu perniagaan dan ekonomi dalam konteks negara ini.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"speech/record1.mp3\"},\n",
    "        {\"type\": \"text\", \"text\": \"apa isu\"},\n",
    "    ]},\n",
    "]\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios = []\n",
    "for message in conversation:\n",
    "    if isinstance(message[\"content\"], list):\n",
    "        for ele in message[\"content\"]:\n",
    "            if ele[\"type\"] == \"audio\":\n",
    "                audios.append(librosa.load(ele['audio_url'], \n",
    "                    sr=processor.feature_extractor.sampling_rate)[0]\n",
    "                )\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "    generate_kwargs = dict(\n",
    "        max_new_tokens=128,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        temperature=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.05,\n",
    "        streamer=streamer,\n",
    "        **inputs,\n",
    "    )\n",
    "    generation_output = ori_model.generate(**generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b118390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-26 06:44:01,410] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00e1c1b1de142aa8e67ca32a580f9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfa4af5f7ac44b0b3208058d4756df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2708f2322252462392b74093598c44e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20442e6f9185415d89e6aea131ba37d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1756faceb1724871bea3a87e03fe6d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/malaysia-ai/Malaysian-Qwen2-Audio-7B-Instruct/commit/bcf6f54485f2f53ec5592e4b167024c5421c5b2e', commit_message='Upload Qwen2AudioForConditionalGeneration', commit_description='', oid='bcf6f54485f2f53ec5592e4b167024c5421c5b2e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/malaysia-ai/Malaysian-Qwen2-Audio-7B-Instruct', endpoint='https://huggingface.co', repo_type='model', repo_id='malaysia-ai/Malaysian-Qwen2-Audio-7B-Instruct'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_model.push_to_hub('malaysia-ai/Malaysian-Qwen2-Audio-7B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71f3f73c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c8422d44114c0dbfb8e801e8ec4e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac11a591d41b41d7b02630bc4759b021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/12.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/malaysia-ai/Malaysian-Qwen2-Audio-7B-Instruct/commit/c7730ac11e06bf3597017b448f9db14e0c9367e0', commit_message='Upload processor', commit_description='', oid='c7730ac11e06bf3597017b448f9db14e0c9367e0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/malaysia-ai/Malaysian-Qwen2-Audio-7B-Instruct', endpoint='https://huggingface.co', repo_type='model', repo_id='malaysia-ai/Malaysian-Qwen2-Audio-7B-Instruct'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.push_to_hub('malaysia-ai/Malaysian-Qwen2-Audio-7B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2171e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2AudioForConditionalGeneration(\n",
       "  (audio_tower): Qwen2AudioEncoder(\n",
       "    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (embed_positions): Embedding(1500, 1280)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Qwen2AudioEncoderLayer(\n",
       "        (self_attn): Qwen2AudioSdpaAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): GELUActivation()\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (avg_pooler): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  )\n",
       "  (multi_modal_projector): Qwen2AudioMultiModalProjector(\n",
       "    (linear): Linear(in_features=1280, out_features=4096, bias=True)\n",
       "  )\n",
       "  (language_model): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(156032, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=156032, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b1ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
