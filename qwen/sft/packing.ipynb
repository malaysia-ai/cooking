{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a51f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2-1.5B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7853d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from streaming import MDSWriter\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from streaming import LocalDataset\n",
    "import streaming\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "columns = {\n",
    "    'input_ids': 'uint32',\n",
    "    'position_ids': 'uint32',\n",
    "    'attention_mask': 'uint32',\n",
    "}\n",
    "hashes = 'sha1', 'xxh64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fff1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1294946"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine = []\n",
    "with open('combined-malaysian-sft.jsonl') as fopen:\n",
    "    for l in fopen:\n",
    "        l = json.loads(l)\n",
    "        combine.append(l)\n",
    "\n",
    "len(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7cfa079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def collator(batch, batch_position_ids):\n",
    "    input_ids = []\n",
    "    position_ids = []\n",
    "    masks = []\n",
    "    for i in range(len(batch)):\n",
    "        l = len(batch[i])\n",
    "        input_ids.extend(batch[i])\n",
    "        position_ids.extend(batch_position_ids[i])\n",
    "        masks.append(l)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(input_ids).astype(np.uint32),\n",
    "        'position_ids': np.array(position_ids).astype(np.uint32),\n",
    "        'attention_mask': np.array(masks).astype(np.uint32),\n",
    "    }\n",
    "\n",
    "def slice_and_balance(nested_list, size):\n",
    "    first = []\n",
    "    balance = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sublist in nested_list:\n",
    "        if current_size < size:\n",
    "            remaining_space = size - current_size\n",
    "            if len(sublist) <= remaining_space:\n",
    "                first.append(sublist)\n",
    "                current_size += len(sublist)\n",
    "            else:\n",
    "                first.append(sublist[:remaining_space])\n",
    "                balance.append(sublist[remaining_space:])\n",
    "                current_size = size\n",
    "        else:\n",
    "            balance.append(sublist)\n",
    "    \n",
    "    return first, balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade760e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def loop(files, block_size = 3072):\n",
    "    rows, index = files\n",
    "    out_root = f'tokenized-4k/tokenized-{index}'\n",
    "    os.system(f'rm -rf {out_root}')\n",
    "    count = 0\n",
    "    temp = []\n",
    "    position_ids = []\n",
    "    last_block, last_position_block = None, None\n",
    "    with MDSWriter(out=out_root, columns=columns, compression=None, hashes=hashes) as out:\n",
    "        for row in tqdm(rows):\n",
    "            prompt = tokenizer.apply_chat_template(row, tokenize=False)\n",
    "            outputs = tokenizer(prompt, add_special_tokens = False)\n",
    "            temp.append(outputs['input_ids'])\n",
    "            position_ids.append(range(len(outputs['input_ids'])))\n",
    "            count += len(outputs['input_ids'])\n",
    "            while count >= block_size:\n",
    "                block, temp = slice_and_balance(temp, block_size)\n",
    "                block_position, position_ids = slice_and_balance(position_ids, block_size)\n",
    "                count = count - block_size\n",
    "                o = collator(block, block_position)\n",
    "                last_block = block\n",
    "                last_position_block = block_position\n",
    "                out.write(o)\n",
    "                \n",
    "        block, _ = slice_and_balance(last_block, block_size - count)\n",
    "        block_position, _ = slice_and_balance(last_position_block, block_size - count)\n",
    "\n",
    "        block.extend(temp)\n",
    "        block_position.extend(position_ids)\n",
    "\n",
    "        o = collator(block, block_position)\n",
    "        if len(o['input_ids']) == block_size:\n",
    "            out.write(o)\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c7bbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:26<00:00, 576.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:52<00:00, 445.87it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [02:20<00:00, 354.72it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:27<00:00, 1803.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [02:24<00:00, 347.12it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:25<00:00, 1956.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:29<00:00, 1710.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [03:32<00:00, 234.77it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [03:35<00:00, 231.60it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [03:59<00:00, 208.42it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [04:21<00:00, 190.85it/s]\n",
      " 67%|██████████████████████████████████████████████████████▊                           | 33421/50000 [02:04<01:35, 173.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [04:38<00:00, 179.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:00<00:00, 827.70it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████▉| 49967/50000 [03:20<00:00, 256.24it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [03:20<00:00, 249.00it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [03:32<00:00, 235.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [02:43<00:00, 305.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [07:39<00:00, 108.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [03:54<00:00, 213.20it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [04:34<00:00, 182.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [04:47<00:00, 173.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 44946/44946 [02:40<00:00, 280.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [03:19<00:00, 251.11it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [04:53<00:00, 170.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50000/50000 [04:16<00:00, 194.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from multiprocess import Pool\n",
    "import mp\n",
    "\n",
    "chunks = mp.chunks(combine, 50000)\n",
    "pool = Pool(10)\n",
    "pooled = pool.map(loop, chunks)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d10275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenized-4k/tokenized-0',\n",
       " 'tokenized-4k/tokenized-1',\n",
       " 'tokenized-4k/tokenized-2',\n",
       " 'tokenized-4k/tokenized-3',\n",
       " 'tokenized-4k/tokenized-4',\n",
       " 'tokenized-4k/tokenized-5',\n",
       " 'tokenized-4k/tokenized-6',\n",
       " 'tokenized-4k/tokenized-7',\n",
       " 'tokenized-4k/tokenized-8',\n",
       " 'tokenized-4k/tokenized-9',\n",
       " 'tokenized-4k/tokenized-10',\n",
       " 'tokenized-4k/tokenized-11',\n",
       " 'tokenized-4k/tokenized-12',\n",
       " 'tokenized-4k/tokenized-13',\n",
       " 'tokenized-4k/tokenized-14',\n",
       " 'tokenized-4k/tokenized-15',\n",
       " 'tokenized-4k/tokenized-16',\n",
       " 'tokenized-4k/tokenized-17',\n",
       " 'tokenized-4k/tokenized-18',\n",
       " 'tokenized-4k/tokenized-19',\n",
       " 'tokenized-4k/tokenized-20',\n",
       " 'tokenized-4k/tokenized-21',\n",
       " 'tokenized-4k/tokenized-22',\n",
       " 'tokenized-4k/tokenized-23',\n",
       " 'tokenized-4k/tokenized-24',\n",
       " 'tokenized-4k/tokenized-25']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = sorted(glob('tokenized-4k/tokenized-*'), key = lambda x: int(x.split('-')[-1]))\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85e34dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf packing-4k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ed8dea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 11292/11292 [00:00<00:00, 13251.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 12072/12072 [00:00<00:00, 14769.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 19900/19900 [00:01<00:00, 12981.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 14324/14324 [00:01<00:00, 12911.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 15339/15339 [00:01<00:00, 13637.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 17845/17845 [00:01<00:00, 13796.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 29813/29813 [00:01<00:00, 16732.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 19563/19563 [00:01<00:00, 16388.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7981/7981 [00:00<00:00, 12956.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 6310/6310 [00:00<00:00, 12933.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1942/1942 [00:00<00:00, 12470.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1849/1849 [00:00<00:00, 9614.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 2093/2093 [00:00<00:00, 21639.90it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9146/9146 [00:00<00:00, 12235.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 13356/13356 [00:01<00:00, 13057.98it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 13040/13040 [00:00<00:00, 15344.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 12633/12633 [00:00<00:00, 15359.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 17453/17453 [00:00<00:00, 18285.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 16924/16924 [00:01<00:00, 16248.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 14389/14389 [00:00<00:00, 18475.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4200/4200 [00:00<00:00, 21369.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 22462/22462 [00:01<00:00, 16880.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9195/9195 [00:00<00:00, 18968.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 16741/16741 [00:00<00:00, 18155.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 32586/32586 [00:01<00:00, 17203.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 14494/14494 [00:00<00:00, 15903.06it/s]\n"
     ]
    }
   ],
   "source": [
    "with MDSWriter(out='packing-4k', columns=columns, compression=None, hashes=hashes) as out:\n",
    "    for f in folders:\n",
    "        try:\n",
    "            dataset = LocalDataset(local=f)\n",
    "            for i in tqdm(range(len(dataset))):\n",
    "                out.write(dataset[i])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c71cb7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.096525824"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = LocalDataset('packing-4k')\n",
    "(len(dataset) * 3072) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed1b8c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' dan nada keseluruhan teks.<|im_end|>\\n<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\nMenggunakan Pempamer Stanford, huraikan ayat \"She ate the banana\" menjadi pokok kebergantungan, pokok konstituensi dan pokok pelabelan peranan semantik. Selain itu, berikan tag Bahagian-dari-Petikan untuk setiap perkataan dalam ayat dan lemma untuk setiap kata kerja. Akhir sekali, buat visualisasi dari pohon penghuraian menggunakan Graphviz., sentiasa respond dalam bahasa indonesia<|im_end|>\\n<|im_start|>assistant\\nUntuk melakukan parse kalimat \"She ate the banana\" menggunakan pemparser Stanford, kita perlu mengunduh terlebih dahulu pemparser tersebut dari website resmi Stanford. Setelah itu, langkah-langkah yang perlu diambil adalah sebagai berikut:\\n\\n1. Import library pemparser Stanford dan buat objek untuk melakukan parse:\\n\\n```\\nfrom stanfordcorenlp import StanfordCoreNLP\\n\\nnlp = StanfordCoreNLP(\\'path/to/stanford-corenlp\\', lang=\\'en\\')\\n```\\n\\n2. Parse kalimat menggunakan library pemparser Stanford:\\n\\n```\\nsentence = \\'She ate the banana\\'\\ndependency_tree = nlp.dependency_parse(sentence)\\nconstituency_tree = nlp.parse(sentence)\\nsemantic_tree = nlp.semgrex(sentence, \\'{tag:/VB.*/}=action > nsubj=subject > dobj=object\\')\\n\\npos_tags = nlp.pos_tag(sentence)\\nlemmas = nlp.lemmatize(sentence)\\n```\\n\\n3. Tampilkan hasil parsing dalam bentuk pohon dengan Graphviz:\\n\\n```\\nfrom graphviz import Digraph\\n\\ndef build_dependency_tree(dependency_tree):\\n    graph = Digraph()\\n    for i, node in enumerate(dependency_tree):\\n        graph.node(str(i + 1), label=node[0])\\n        if node[2] != 0:\\n            graph.edge(str(node[2]), str(i + 1), label=node[1])\\n    return graph\\n\\ndef build_constituency_tree(constituency_tree):\\n    graph = Digraph()\\n    graph.node(\\'ROOT\\', label=\\'ROOT\\')\\n    for subtree in constituency_tree.subtrees():\\n        if subtree.label() != \\'ROOT\\':\\n            graph.node(str(id(subtree)), label=subtree.label())\\n            graph.edge(str(id(subtree.get_parent())), str(id(subtree)))\\n    return graph\\n\\ndef build_semantic_tree(semantic_tree):\\n    graph = Digraph()\\n    for match in semantic_tree[\\'sentences\\'][0][\\'0\\'][\\'matches\\']:\\n        action = match[\\'$action\\'][\\'category\\']\\n        subject = match[\\'$subject\\'][\\'text\\']\\n        obj = match[\\'$object\\'][\\'text\\']\\n        graph.node(action, label=action)\\n        graph.node(subject, label=subject)\\n        graph.node(obj, label=obj)\\n        graph.edge(action, subject, label=\\'nsubj\\')\\n        graph.edge(action, obj, label=\\'dobj\\')\\n    return graph\\n\\ndependency_graph = build_dependency_tree(dependency_tree)\\nconstituency_graph = build_constituency_tree(constituency_tree)\\nsemantic_graph = build_semantic_tree(semantic_tree)\\n\\ndependency_graph.view()\\nconstituency_graph.view()\\nsemantic_graph.view()\\n```\\n\\n4. Tampilkan hasil parsing dalam bentuk pos tags dan lemma untuk setiap kata:\\n\\n```\\nfor word, pos_tag in zip(pos_tags, lemmas):\\n    print(f\\'{word[0]} ({word[1]}): {pos_tag}\\')\\n```\\n\\nHasil output parsing kalimat \"She ate the banana\" akan menjadi sebagai berikut:\\n\\nDependency Tree:\\n\\n```\\ndigraph {\\n\\t1 [label=\"She\"]\\n\\t2 [label=\"ate\"]\\n\\t3 [label=\"the\"]\\n\\t4 [label=\"banana\"]\\n\\t2 -> 1 [label=\"nsubj\"]\\n\\t2 -> 4 [label=\"dobj\"]\\n\\t4 -> 3 [label=\"det\"]\\n}\\n```\\n\\nConstituency Tree:\\n\\n```\\ndigraph {\\n\\tROOT -> S\\n\\tS -> NP\\n\\tS -> VP\\n\\tNP -> PRP\\n\\tVP -> VBD\\n\\tVP -> NP\\n\\tNP -> DT\\n\\tNP -> NN\\n\\tPRP [label=\"She\"]\\n\\tVBD [label=\"ate\"]\\n\\tDT [label=\"the\"]\\n\\tNN [label=\"banana\"]\\n\\tS [label=\"S\"]\\n\\tNP [label=\"NP\"]\\n\\tVP [label=\"VP\"]\\n}\\n```\\n\\nSemantic Tree:\\n\\n```\\ndigraph {\\n\\tate [label=\"ate\"]\\n\\tShe [label=\"She\"]\\n\\tbanana [label=\"banana\"]\\n\\tate -> She [label=\"nsubj\"]\\n\\tate -> banana [label=\"dobj\"]\\n}\\n```\\n\\nPos Tags dan Lemmas:\\n\\n```\\nShe (PRP): she\\nate (VBD): eat\\nthe (DT): the\\nbanana (NN): banana\\n```<|im_end|>\\n<|im_start|>system\\nawak adalah chatbot yang respond dalam bahasa indonesia<|im_end|>\\n<|im_start|>user\\nBolehkah anda menulis soalan SQL menggunakan LaTeX untuk mencari purata gaji pekerja di setiap jabatan syarikat? Anggaplah data pekerja syarikat disimpan dalam jadual bernama \"pekerja\" dengan lajur \"nama\", \"gaji\" dan \"jabatan\".<|im_end|>\\n<|im_start|>assistant\\nTidak dapat. LaTeX merupakan bahasa markup untuk menulis dokumen dan bukan untuk menulis kueri SQL. Sebaliknya, kamu harus menggunakan bahasa pemrograman atau aplikasi untuk melakukan query SQL sesuai dengan pengaturan tabel pada database yang digunakan. Berikut adalah contoh kueri SQL untuk mencari rata-rata gaji karyawan di setiap departemen perusahaan berdasarkan data karyawan pada tabel \"karyawan\":\\n```\\nSELECT departemen, AVG(gaji) as rata_gaji\\nFROM karyawan\\nGROUP BY departemen;\\n```\\nKueri ini akan mengelompokkan data berdasarkan departemen dan menghasilkan nilai rata-rata gaji karyawan di setiap departemen.<|im_end|>\\n<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\nalways reply in indon\\n\\nAnda seorang peminat sastera dan baru sahaja membaca novel klasik \"Pride and Prejudice\" oleh Jane Austen. anda ingin membuat representasi visual hubungan antara watak menggunakan kod.\\nAnda telah memutuskan untuk menggunakan graf terarah untuk mewakili watak dan hubungan mereka. Setiap watak akan diwakili oleh nod, dan tepi antara nod akan mewakili hubungan antara watak.\\nTugas anda adalah menulis fungsi yang mengambil senarai hubungan karakter dan mengeluarkan graf terarah yang sesuai.\\nSenarai input akan dalam format berikut:\\n```\\n[\\n    (\"Elizabeth Bennet\", \"Jane Bennet\"),\\n    (\"Elizabeth Bennet\", \"Mr. Darcy\"),\\n    (\"Elizabeth Bennet\", \"Mr. Collins\"),\\n    (\"Elizabeth Bennet\", \"Charlotte Lucas\"),\\n    (\"Mr. Darcy\", \"Mr. Bingley\"),\\n    (\"Mr. Darcy\", \"Lady Catherine\"),\\n    (\"Mr. Collins\", \"Charlotte Lucas\"),\\n    (\"Mr. Bingley\", \"Jane Bennet\"),\\n    (\"Mr. Bingley\", \"Mr. Collins\")\\n]\\n```\\nSetiap tupel dalam senarai mewakili hubungan antara dua watak. Elemen pertama tupel adalah nama watak yang memulakan hubungan, dan elemen kedua tupel adalah nama watak yang menerima hubungan.\\nFungsi anda mesti mengeluarkan graf terarah yang diwakili sebagai kamus, di mana setiap kunci adalah nod (nama watak) dan nilai yang sesuai adalah senarai nod (watak) yang berkaitan dengannya.\\nContohnya, output untuk senarai input di atas mestilah:\\n```\\n{\\n    \"Elizabeth Bennet\": [\"Jane Bennet\", \"Mr. Darcy\", \"Mr. Collins\", \"Charlotte Lucas\"],\\n    \"Jane Bennet\": [],\\n    \"Mr. Darcy\": [\"Mr. Bingley\", \"Lady Catherine\"],\\n    \"Mr. Collins\": [\"Charlotte Lucas\"],\\n    \"Charlotte Lucas\": [],\\n    \"Mr. Bingley\": [\"Jane Bennet\", \"Mr. Collins\"],\\n    \"Lady Catherine\": []\\n}\\n```\\nDengan menggunakan graf ini, anda boleh dengan mudah menganalisis hubungan antara watak dan mengenal pasti sambungan dan pengaruh utama.<|im_end|>\\n<|im_start|>assistant\\nUntuk menyelesaikan tugas ini, kita perlu memproses daftar input untuk menghasilkan grafik berarah yang sesuai. Kita bisa menggunakan kamus Python untuk merepresentasikan grafik, dengan setiap kunci sebagai node dan nilai yang sesuai adalah daftar node yang terhubung dengannya. Berikut adalah langkah-langkah untuk menyelesaikan tugas ini secara rinci:\\n\\n1. Inisialisasi kamus kosong yang akan berisi grafik berarah.\\n```\\ngraph = {}\\n```\\n\\n2. Loop melalui daftar input dan tambahkan node dan tepi ke grafik. Untuk setiap tupel dalam daftar input, kita perlu menambahkan dua node ke grafik: satu untuk karakter yang memulai hubungan dan satu lagi untuk karakter yang menerima hubungan. Jika node belum ada dalam grafik, tambahkan node baru dengan daftar kosong sebagai nilai.\\n```\\nfor edge in edges:\\n    # Add starting character to graph if not already in it\\n    if edge[0] not in graph:\\n        graph[edge[0]] = []\\n    # Add ending character to graph if not already in it\\n    if edge[1] not in graph:\\n        graph[edge[1]] = []\\n    # Add edge to graph by appending ending character to starting character\\'s list\\n    graph[edge[0]].append(edge[1])\\n```\\n\\n3. Kembalikan grafik yang dihasilkan.\\n```\\nreturn graph\\n```\\n\\nDengan menggunakan pendekatan ini, kita dapat menghasilkan grafik berarah yang mewakili hubungan antara karakter dalam bentuk yang mudah diproses dan dianalisis.<|im_end|>\\n<|im_start|>system\\nyou are an assistant that always reply in indon<|im_end|>\\n<|im_start|>user\\nBerikut adalah teka-teki untuk anda: Bolehkah anda membuat algoritma yang boleh mengkategorikan senarai barangan kepada beberapa kategori yang tidak diketahui? Barangan dalam senarai ialah Pisang, Anggur, Mangga, Epal, Nanas, Pepaya dan Oren. Walau bagaimanapun, anda tidak tahu berapa banyak kategori yang ada sebelum ini. Untuk menangani cabaran ini lebih sukar lagi, kategori bukan berdasarkan syarat yang telah ditetapkan sebelum ini. Bolehkah anda mencipta kod yang boleh mengkategorikan barang dengan cekap ke dalam pelbagai kategori yang tidak diketahui berdasarkan sifat unik mereka? Ingat bahawa kod mesti dioptimumkan untuk mengendalikan jumlah item yang besar. Semoga berjaya!<|im_end|>\\n<|im_start|>assistant\\nAlgoritma untuk Mengkategorikan Daftar Barang:\\n\\nLangkah 1: Inisialisasi\\n- Buat sebuah variabel kosong bernama \"kategori\"\\n- Buat sebuah variabel kosong bernama \"daftar_kategori\"\\n\\nLangkah 2: Looping\\n- Looping melalui setiap barang dalam daftar\\n- Untuk setiap barang, lakukan langkah berikut:\\n    - Buat sebuah variabel bernama \"ditemukan\"\\n    - Looping melalui setiap kategori yang telah ada dalam \"daftar_kategori\"\\n    - Untuk setiap kategori, lakukan langkah berikut:\\n        - Jika barang memiliki sifat unik yang sama dengan kategori tersebut, tambahkan barang ke kategori tersebut dan ubah nilai variabel \"ditemukan\" menjadi true\\n    - Jika variabel \"ditemukan\" masih false setelah looping melalui semua kategori, maka buat kategori baru dan tambahkan barang ke kategori tersebut\\n\\nLangkah 3: Selesai\\n- Hasil kategorisasi akan tersedia dalam variabel \"daftar_kategori\"\\n\\nBerikut adalah contoh implementasi algoritma ini dalam bahasa Python:\\n\\n```\\n# inisialisasi\\nbarang = [\\'Pisang\\', \\'Anggur\\', \\'Mangga\\', \\'Apel\\', \\'Nanas\\', \\'Pepaya\\', \\'Jeruk\\']\\nkategori = []\\ndaftar_kategori = []\\n\\n# looping\\nfor b in barang:\\n    ditemukan = False\\n    for k in kategori:\\n        if b.startswith(k):\\n            daftar_kategori[k].append(b)\\n            ditemukan = True\\n            break\\n    if not ditemukan:\\n        kategori_baru = b[:2]\\n        kategori.append(kategori_baru)\\n        daftar_kategori[kategori_baru] = [b]\\n\\n# hasil\\nfor k, v in daftar_kategori.items():\\n    print(k + \\': \\', end=\\'\\')\\n    print(v)\\n```\\n\\nPenjelasan implementasi:\\n\\n- Pertama, kita inisialisasi variabel \"kategori\" dan \"daftar_kategori\" yang masih kosong\\n- Looping dilakukan untuk setiap barang dalam daftar. Variabel \"ditemukan\" digunakan untuk mencatat apakah barang telah ditemukan dalam kategori yang ada.\\n- Looping kedua dilakukan untuk setiap kategori yang telah ada. Jika barang memiliki sifat unik yang sama dengan kategori tersebut (yaitu, jika nama barang dimulai dengan nama kategori), maka barang ditambahkan ke kategori tersebut dan variabel \"ditemukan\" diubah menjadi true.\\n- Jika variabel \"ditemukan\" masih false setelah looping melalui semua kategori, maka kita membuat kategori baru dengan mengambil dua huruf pertama dari nama barang dan menambahkannya ke \"kategori\". Barang tersebut kemudian ditambahkan ke daftar di kategori baru yang dibuat di \"daftar_kategori\".\\n- Setelah looping selesai, hasil kategorisasi tersedia dalam variabel \"daftar_kategori\". Looping terakhir digunakan untuk mencetak hasil kategorisasi dalam format yang mudah dibaca (kategori diikuti oleh daftar barang di dalamnya).<|im_end|>\\n<|im_start|>system\\nYou'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[-3]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32d65447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' are a helpful assistant<|im_end|>\\n<|im_start|>user\\nu always reply in indonesian\\n\\nApakah formula untuk mengira penurunan peratusan permintaan ruang pejabat disebabkan oleh kesan kerja jarak jauh, seperti yang dibincangkan dalam artikel ini: [Kesan Kerja Jauh ke atas Ruang Pejabat Tradisional](https://www.bbc.com/news/business-54050430)? Artikel tersebut menyebut bahawa pandemik COVID-19 telah mempercepatkan trend ke arah kerja jarak jauh, mengakibatkan penurunan permintaan ruang pejabat dan peningkatan tahap kosong. Untuk menyesuaikan dengan trend ini, beberapa syarikat sedang meneroka cara baru bekerja, seperti jadual fleksibel dan tempat kerja bersama. Formula penurunan peratusan dikira seperti berikut:\\n```\\nPeratus penurunan = ((nilai awal - nilai akhir) / nilai awal) * 100\\n```\\nMenyimpulkan permintaan awal ruang pejabat sebelum wabak adalah 100 dan permintaan semasa ialah 75, maka peratus penurunan akan dikira seperti berikut:\\n```\\nPeratusan penurunan = ((100 - 75) / 100) * 100 = 25%\\n```\\nDengan itu, peratusan penurunan permintaan ruang pejabat disebabkan oleh kesan kerja jarak jauh adalah 25%.<|im_end|>\\n<|im_start|>assistant\\nThe formula for calculating the percentage decrease in office space demand due to remote work, as discussed in the article \"The Impact of Remote Work on Traditional Office Spaces\" is as follows:\\n\\n```\\nPercentage decrease = ((Initial value - Current value) / Initial value) * 100\\n```\\n\\nThe article states that the COVID-19 pandemic has accelerated the trend towards remote work, resulting in a decrease in office space demand and an increase in vacancy rates. To adapt to this trend, some companies are exploring new ways of working, such as flexible schedules and shared workspaces.\\n\\nIf we assume that the initial demand for office space before the pandemic was 100 and the current demand is 75, then the percentage decrease can be calculated as follows:\\n\\n```\\nPercentage decrease = ((100 - 75) / 100) * 100 = 25%\\n```\\n\\nTherefore, the percentage decrease in office space demand due to the influence of remote work is 25%.<|im_end|>\\n<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\nBuat soalan SQL untuk mengambil pengenalan pelanggan dan nama depan pelanggan yang berumur di bawah 30 tahun., u always reply in indonesian<|im_end|>\\n<|im_start|>assistant\\nBerikut adalah kueri SQL untuk mengambil identifikasi pelanggan dan nama depan pelanggan yang berusia di bawah 30:\\n\\n```\\nSELECT customer_id, first_name\\nFROM customers\\nWHERE DATE_PART(\\'year\\', age(birthdate)) < 30;\\n```\\n\\nPenjelasan:\\n\\n1. `SELECT customer_id, first_name`: Kueri ini akan mengambil dua kolom yaitu `customer_id` dan `first_name`.\\n\\n2. `FROM customers`: Kueri ini akan mengambil data dari tabel `customers`.\\n\\n3. `WHERE DATE_PART(\\'year\\', age(birthdate)) < 30`: Kueri ini akan memfilter data dengan kondisi di mana umur pelanggan (`birthdate`) yang dihitung dalam tahun (`DATE_PART(\\'year\\', age(birthdate))`) lebih kecil dari 30 tahun.\\n\\nDengan menggunakan kueri SQL di atas, anda akan mendapatkan hasil yang berisi informasi identifikasi pelanggan dan nama depannya yang berusia di bawah 30.<|im_end|>\\n<|im_start|>system\\nAnda adalah chatbot yang merespon dalam indon<|im_end|>\\n<|im_start|>user\\nBerdasarkan data XML yang diberikan, apakah kategori emosi utama dan tahap keamatan bagi teks berikut: \"Saya tidak boleh percaya saya kehilangan pekerjaan saya, ini sangat menjengkelkan\"? Sila tulis soalan SQL untuk mengekstrak maklumat ini daripada data XML yang diberikan.<|im_end|>\\n<|im_start|>assistant\\nQuery SQL untuk mengekstrak informasi kategori emosi utama dan tingkat intensitas dari teks yang diberikan dari data XML adalah sebagai berikut:\\n\\n```\\nSELECT\\n    ExtractValue(xml_data, \\'//category\\') AS Emotion_Category,\\n    ExtractValue(xml_data, \\'//intensity\\') AS Emotion_Intensity\\nFROM\\n    your_table_name\\nWHERE\\n    ExtractValue(xml_data, \\'//text\\') = \\'Saya tidak bisa percaya saya kehilangan pekerjaan saya, ini sangat menjengkelkan\\'\\n```\\n\\nPenjelasan:\\n\\n1. Pertama, kita menggunakan fungsi `ExtractValue()` untuk mengekstrak nilai dari data XML.\\n2. Kemudian, kita menentukan `//category` dan `//intensity` sebagai jalur yang akan mengekstrak nilai kategori emosi utama dan tingkat intensitas secara berturut-turut dari elemen `<emotion>` dalam XML data.\\n3. Selanjutnya, kita menentukan nama tabel anda di `FROM` klausa dan `xml_data` kolom yang berisi data XML di mana informasi emosi harus diekstrak.\\n4. Terakhir, kita memasukkan kondisi `WHERE` pada elemen `<text>` dalam data XML untuk mencari teks yang diberikan `\"Saya tidak bisa percaya saya kehilangan pekerjaan saya, ini sangat menjengkelkan\"`.<|im_end|>\\n<|im_start|>system\\nyou are a chatbot that always respond in indonesian<|im_end|>\\n<|im_start|>user\\nAnggap anda mempunyai fungsi\\'my_func(lst1, lst2)\\' yang mengambil dua senarai sebagai argumen. Senarai pertama mengandungi rentetan, dan senarai kedua mengandungi integer. Apakah output dijangka dari my_func ([\"apple\", \"banana\", \"cherry\"], [2, 5, 8]) jika output mestilah kamus dengan huruf pertama setiap rentetan dalam senarai pertama sebagai kunci dan jumlah integer dalam senarai kedua sebagai nilai?<|im_end|>\\n<|im_start|>assistant\\nJawaban:\\n\\nJika kita memasukkan daftar pertama yang terdiri dari [\"apple\", \"banana\", \"cherry\"] dan daftar kedua yang terdiri dari [2, 5, 8] sebagai argumen fungsi my_func, maka keluaran yang diharapkan adalah sebuah kamus dengan huruf pertama dari setiap string di daftar pertama sebagai kunci dan jumlah bilangan bulat di daftar kedua sebagai nilai.\\n\\nJadi, kamus keluarannya akan terlihat seperti ini:\\n\\n```\\n{ \"a\": 2, \"b\": 5, \"c\": 8 }\\n```\\n\\nPenjelasan:\\n\\nKita mulai dengan membuat sebuah kamus kosong dengan menggunakan tanda kurung kurawal {}. Kemudian, untuk setiap elemen di daftar pertama, kita mengambil huruf pertamanya dengan menggunakan indeks 0 dan kita gunakan huruf tersebut sebagai kunci dalam kamus yang kita buat sebelumnya. Selanjutnya, kita mencari nilai untuk setiap kunci tersebut di daftar kedua dengan menggunakan indeks yang sesuai. Kita menambahkan nilai-nilai tersebut sebagai nilai kunci dalam kamus.\\n\\nSetelah mengulang prosesnya untuk setiap elemen di daftar pertama dan daftar kedua, kita akan memiliki kamus yang terdiri dari huruf pertama setiap string di daftar pertama sebagai kunci dan jumlah bilangan bulat di daftar kedua sebagai nilai. Kamus ini akan menjadi keluaran dari fungsi my_func.<|im_end|>\\n<|im_start|>system\\nalways reply in indon<|im_end|>\\n<|im_start|>user\\nAlice mempunyai taman besar yang berbentuk segi empat sempurna. Dia ingin memasang air pancut berbentuk bulatan di tengah-tengah taman. Jari air pancut mestilah menyentuh keempat-empat sudut taman. Jika keluasan taman ialah 400 meter persegi, berapakah jari air pancut itu? Bulatkan jawapan anda kepada dua perpuluhan terdekat.<|im_end|>\\n<|im_start|>assistant\\nUntuk menyelesaikan masalah ini, kita dapat menggunakan persamaan berikut: \\n\\nLuas taman = (sisi)^2\\n\\nKarena taman tersebut berbentuk persegi sempurna, maka setiap sisinya sama panjang. Oleh karena itu, jika luas taman adalah 400 meter persegi, maka setiap sisinya adalah akar dari 400, yaitu:\\n\\nsisi = akar(400) = 20 meter\\n\\nKetika air mancur berbentuk lingkaran, jari-jarinya akan menjadi radius lingkaran. Karena jari-jari harus menyentuh keempat sudut taman, maka jarak dari tengah lingkaran ke setiap sudut taman harus sama dengan panjang jari-jari. Kita dapat menggambar taman dan air mancur seperti gambar berikut:\\n\\n```\\n_______________\\n|     .       |\\n|       *     |\\n|             |\\n|     *       |\\n|___ . _______|\\n```\\n\\nDi gambar tersebut, simbol \"*\" menunjukkan titik pusat lingkaran air mancur (yang harus berada di tengah taman), dan simbol \".\" menunjukkan sudut taman yang harus disentuh oleh jari-jari. Jarak dari pusat lingkaran ke sudut taman adalah sama dengan jari-jari.\\n\\nUntuk mencari panjang jari-jari, kita perlu mencari jarak dari pusat lingkaran ke sudut taman. Kita dapat menggunakan teorema Pythagoras untuk menghitung jarak tersebut:\\n\\nJarak = akar((setengah sisi)^2 + (setengah sisi)^2)\\n\\nJarak = akar(2 x (setengah sisi)^2)\\n\\nJarak = setengah sisi x akar(2)\\n\\nJarak = 20 x akar(2)\\n\\nKarena kita perlu membulatkan jawaban ke dua desimal terdekat, maka kita dapat mengalikan jarak tersebut dengan 100 kemudian membulatkan hasilnya, dan kemudian membagi hasilnya dengan 100 lagi:\\n\\nJari-jari = Jarak = 20 x akar(2) ≈ 28.28\\n\\nJadi, jari-jari air mancur tersebut sekitar 28,28 meter (bulatkan ke dua desimal terdekat).<|im_end|>\\n<|im_start|>system\\nyou are an assistant that always reply in indon<|im_end|>\\n<|im_start|>user\\nBolehkah anda memecahkan teka-teki molekul ini? Berikan dua perbezaan utama antara komposisi kimia dan fungsi biologi laluan sintesis DNA dan RNA. Untuk menambah keseronokan pengaturcaraan, terangkan bagaimana proses transkripsi DNA kepada RNA digambarkan dalam bahasa swift, dengan menyoroti peringkat penting dalam proses ini. Tetapi tunggu dulu, ada lagi! Bolehkah anda juga mengisi kekosongan dalam teka-teki molekul ini dengan memadankan jenis RNA yang berbeza dengan fungsi mereka yang sesuai dalam proses transkripsi? Lihat jadual di bawah untuk petunjuk:\\n| Jenis RNA | Fungsi |\\n| --- | --- |\\n| mRNA | _________ |\\n| tRNA | _________ |\\n| rRNA | _________ |\\nSelamat menjalankan teka-teki!<|im_end|>\\n<|im_start|>assistant\\nPerbedaan kunci antara komposisi kimia dan fungsi biologis dari jalur sintesis DNA dan RNA adalah sebagai berikut:\\n\\n1. Komposisi kimia: DNA terdiri dari dua untai nukleotida yang saling berkelit membentuk sebuah heliks ganda, sedangkan RNA hanya terdiri dari satu untai nukleotida. Selain itu, nukleotida dalam DNA mengandung basa nitrogen adenin, guanin, sitosin, dan timin, sedangkan RNA mengandung urasil sebagai pengganti timin.\\n\\n2. Fungsi biologis: DNA bertanggung jawab untuk menyimpan informasi genetik dan bertindak sebagai cetakan untuk sintesis RNA. Sementara itu, RNA bertindak sebagai \"pembawa pesan\" untuk mentransfer informasi genetik dari DNA ke ribosom untuk sintesis protein.\\n\\nProses transkripsi DNA menjadi RNA dapat diilustrasikan menggunakan bahasa swift sebagai berikut:\\n\\n```\\n// Mengambil urutan DNA\\nlet dnaSequence = \"ATCGCGTAAGCT\"\\n\\n// Membuat komplementer DNA\\nlet complementDna = dnaSequence\\n    .replacingOccurrences(of: \"A\", with: \"t\")\\n    .replacingOccurrences(of: \"T\", with: \"a\")\\n    .replacingOccurrences(of: \"C\", with: \"g\")\\n    .replacingOccurrences(of: \"G\", with: \"c\")\\n    .uppercased()\\n    \\n// Mengubah DNA menjadi RNA melalui proses transkripsi\\nlet rnaSequence = complementDna\\n    .replacingOccurrences(of: \"T\", with: \"U\")\\n\\nprint(rnaSequence)\\n```\\n\\nTahapan penting dalam proses transkripsi adalah:\\n\\n1. RNA polymerase melekat pada daerah promoter di DNA.\\n2. Enzim RNA polymerase membuka heliks ganda DNA dan membentuk untai RNA dengan menggunakan DNA sebagai cet'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[-2]['input_ids'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
