{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06f667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c19cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM, PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f185437",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('unsloth/Llama-3.2-1B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c75237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a47df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at unsloth/Llama-3.2-1B-Instruct and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "ori_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'unsloth/Llama-3.2-1B-Instruct', tie_word_embeddings=False, torch_dtype = torch.bfloat16,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80330668",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_model.lm_head.weight.data = ori_model.model.embed_tokens.weight.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00736b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = ori_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00d794b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "f = safe_open(f\"lora-embedding-256-llama3.2-1b-small-malaysian/checkpoint-17300/adapter_model.safetensors\", framework=\"pt\", device=0)\n",
    "keys = f.keys()\n",
    "keys = sorted(list(set([k.split('.lora')[0] for k in keys if '.lora' in k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f54db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 114/114 [00:00<00:00, 513.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for k in tqdm(keys):\n",
    "    k_ori = k.replace('base_model.model.', '') + '.weight'\n",
    "    if 'embed_tokens' in k:\n",
    "        post_A = '.lora_embedding_A'\n",
    "        post_B = '.lora_embedding_B'\n",
    "    else:\n",
    "        post_A = '.lora_A.weight'\n",
    "        post_B = '.lora_B.weight'\n",
    "    A = k + post_A\n",
    "    B = k + post_B\n",
    "    \n",
    "    W = state_dict[k_ori]\n",
    "    if 'embed_tokens' not in k:\n",
    "        W = W.t()\n",
    "        \n",
    "    A = f.get_tensor(A).type(W.dtype)\n",
    "    B = f.get_tensor(B).type(W.dtype)\n",
    "    with torch.no_grad():\n",
    "        W.addmm_(A.t(), B.t(), alpha = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d7d7fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ape isu dengan "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/husein/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "najib<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Terima kasih atas pertanyaan anda. Saya akan cuba memberikan jawapan yang terperinci dan jelas.\n",
      "\n",
      "Isu Najib Razak adalah berhubung dengan skandal 1Malaysia Development Ber\n"
     ]
    }
   ],
   "source": [
    "d = [\n",
    "    {'role': 'user', 'content': 'ape isu dengan najib'}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(d, return_tensors = 'pt').to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=50,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.0,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.05,\n",
    "    streamer=streamer\n",
    ")\n",
    "generation_output = ori_model.generate(**generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49cd49d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "kwsp tu ape<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Kwsp merujuk kepada \"Kumpulan Wang Simpanan Pekerja\" yang merupakan skim simpanan persaraan yang ditawarkan oleh kerajaan Malaysia. Ia bertujuan untuk membantu\n"
     ]
    }
   ],
   "source": [
    "d = [\n",
    "    {'role': 'user', 'content': 'kwsp tu ape'}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(d, return_tensors = 'pt').to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=50,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.0,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.05,\n",
    "    streamer=streamer\n",
    ")\n",
    "generation_output = ori_model.generate(**generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52538fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "camne nk code react, reply in jawi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "اونتوق ممبوات کومڤونن React يڠ مڽوکوڠ "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "فوڠسي \"کليک\"، اندا بوليه مڠݢوناکن کوم\n"
     ]
    }
   ],
   "source": [
    "d = [\n",
    "    {'role': 'user', 'content': 'camne nk code react, reply in jawi'}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(d, return_tensors = 'pt').to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=50,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.1,\n",
    "    streamer=streamer\n",
    ")\n",
    "generation_output = ori_model.generate(**generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1686261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "camne nk code react website order kasut, reply in sabah<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Bah, kunuk sia mau kasi tau macam mana mo bikin laman web untuk pesanan kasut pakai React. Tingu saja la:\n",
      "\n",
      "1. Mula-mula, kita mesti pasang dulu\n"
     ]
    }
   ],
   "source": [
    "d = [\n",
    "    {'role': 'user', 'content': 'camne nk code react website order kasut, reply in sabah'}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(d, return_tensors = 'pt').to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=50,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.1,\n",
    "    streamer=streamer\n",
    ")\n",
    "generation_output = ori_model.generate(**generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87ffd1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are going to enter reasoning mode, first you try to think step-by-step in kelantan after that give the final answer.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Derive the joint density function of the DE and SCF biases (f(DE, SCF)) using a Bayesian approach, assuming that the parameters α, β, λ, and θ follow normal distributions with known means and variances, and that the error terms ε and η follow normal distributions with zero means and unknown variances. Use the derived joint density function to compute the expected value of the portfolio return (E(S_t)) and the expected value of the portfolio volatility (E(σ)) across all investors.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Dalam senario ini, kita akan menggunakan pendekatan Bayesian untuk merumuskan taburan kebarangkalian fungsi ketumpatan bersatu (joint distribution) bagi parameter bias DE dan SCF.\n",
      "\n",
      "1. Tentukan taburan normal yang berkaitan dengan:\n",
      "   - Parameter alpha\n",
      "   - Beta\n",
      "   - Lambda\n",
      "   - Theta\n",
      "\n",
      "2. Tetapkan min masing-masing kepada nilai sebenar:\n",
      "\n",
      "   - Mean Alpha: 0\n",
      "   - Min Beta: [0.5, 1]\n",
      "   - Min Lambda: 10\n",
      "   - Min Theta: 3\n",
      "\n",
      "3. Kira sisihan piawai setiap min:\n",
      "\n",
      "   - Sisihan Piawai Alpha: sqrt((sigma^(-2)/N - alpha*alpha)/(sqrt(N)*n)\n",
      "   - Sisihan Piawai Beta: persegi(eta^2/eta*tau*N - beta*beta)/(sqrt(N)*n)\n",
      "   - Sisihan Piawai Lambda: sqrt(beta*beta*N - lambda*lambda/(1 + sigma^(-2)))\n",
      "   - Sisihan Piawai Theta: persegi(eta^2/eta_N - theta_theta/(1 + eta^(-2)))\n",
      "\n",
      "4.\n"
     ]
    }
   ],
   "source": [
    "d = [\n",
    "    {'role': 'user', 'content': 'You are going to enter reasoning mode, first you try to think step-by-step in kelantan after that give the final answer.'},\n",
    "    {'role': 'user', 'content': 'Derive the joint density function of the DE and SCF biases (f(DE, SCF)) using a Bayesian approach, assuming that the parameters α, β, λ, and θ follow normal distributions with known means and variances, and that the error terms ε and η follow normal distributions with zero means and unknown variances. Use the derived joint density function to compute the expected value of the portfolio return (E(S_t)) and the expected value of the portfolio volatility (E(σ)) across all investors.'}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(d, return_tensors = 'pt').to('cuda')\n",
    "generate_kwargs = dict(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=256,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    streamer=streamer\n",
    ")\n",
    "generation_output = ori_model.generate(**generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5111d20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46f66ecee7b4d12b1302b848aa4f07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/malaysia-ai/Malaysian-Llama-3.2-1B/commit/80d1e84cbf1b8f3c72eba6e7e67d2c23f10f1c85', commit_message='Upload tokenizer', commit_description='', oid='80d1e84cbf1b8f3c72eba6e7e67d2c23f10f1c85', pr_url=None, repo_url=RepoUrl('https://huggingface.co/malaysia-ai/Malaysian-Llama-3.2-1B', endpoint='https://huggingface.co', repo_type='model', repo_id='malaysia-ai/Malaysian-Llama-3.2-1B'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('malaysia-ai/Malaysian-Llama-3.2-1B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0f3c1ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6c78832b664291a1ce33fb20d0607b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-07 08:09:46,252] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f978a80ad24d608685b373cbe037df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/malaysia-ai/Malaysian-Llama-3.2-1B/commit/ce72cb898e35d1e0fe399557de9407aad4afb602', commit_message='Upload LlamaForCausalLM', commit_description='', oid='ce72cb898e35d1e0fe399557de9407aad4afb602', pr_url=None, repo_url=RepoUrl('https://huggingface.co/malaysia-ai/Malaysian-Llama-3.2-1B', endpoint='https://huggingface.co', repo_type='model', repo_id='malaysia-ai/Malaysian-Llama-3.2-1B'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_model.push_to_hub('malaysia-ai/Malaysian-Llama-3.2-1B')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
